{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajnya/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/SciQ dataset/train.json', 'r') as rf:\n",
    "    train = json.load(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sequence):\n",
    "    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"') for token in nltk.word_tokenize(sequence)]\n",
    "#     return list(map(lambda x:x.encode('utf8'), tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Glove stuff - Pass pretrained embeddings as input\n",
    "filename = 'data/glove.6B.50d.txt'\n",
    "def loadGloVe(filename):\n",
    "    rev_vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        rev_vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "    return vocab,rev_vocab,embd\n",
    "# vocab,rev_vocab,embd = loadGloVe(filename)\n",
    "# vocab_size = len(vocab)\n",
    "# embedding_dim = len(embd[0])\n",
    "# embedding = np.asarray(embd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_sequences(sequences, pad_tok, max_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "    \"\"\"\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\n",
    "        sequence_padded +=  [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return np.array(sequence_padded), np.array(sequence_length)\n",
    "\n",
    "def pad_sequences(sequences, pad_tok):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "    \"\"\"\n",
    "    max_length = max([len(x) for x in sequences])\n",
    "    sequence_padded, sequence_length = _pad_sequences(sequences, \n",
    "                                            pad_tok, max_length)\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "supportTokens = []\n",
    "questionTokens = []\n",
    "distractor1Tokens = []\n",
    "distractor2Tokens = []\n",
    "distractor3Tokens = []\n",
    "question_embed = []\n",
    "UNK_ID = 2\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab,self.rev_vocab,embd = loadGloVe(filename)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.embedding_dim = len(embd[0])\n",
    "        self.embedding = np.asarray(embd)\n",
    "        self.setup_placeholders()\n",
    "    \n",
    "    def setup_placeholders(self):\n",
    "        self.question_ids = tf.placeholder(tf.int32, shape = [None, None], name = \"question_ids\")\n",
    "        self.support_ids = tf.placeholder(tf.int32, shape = [None, None], name = \"support_ids\")\n",
    "        self.distractor1_ids = tf.placeholder(tf.int32, shape = [None, None], name = \"distractor1_ids\")\n",
    "        self.distractor2_ids = tf.placeholder(tf.int32, shape = [None, None], name = \"distractor2_ids\")\n",
    "        self.distractor3_ids = tf.placeholder(tf.int32, shape = [None, None], name = \"distractor3_ids\")\n",
    "        self.answer_ids = tf.placeholder(tf.int32, shape = [None, None], name = \"answer_ids\")\n",
    "\n",
    "        self.question_lengths = tf.placeholder(tf.int32, shape=[None], name=\"question_lengths\")\n",
    "        self.support_lengths = tf.placeholder(tf.int32, shape = [None], name = \"support_lengths\")\n",
    "        self.distractor1_lengths = tf.placeholder(tf.int32, shape = [None], name = \"distractor1_lengths\")\n",
    "        self.distractor2_lengths = tf.placeholder(tf.int32, shape = [None], name = \"distractor2_lengths\")\n",
    "        self.distractor3_lengths = tf.placeholder(tf.int32, shape = [None], name = \"distractor3_lengths\")\n",
    "        self.answer_lengths = tf.placeholder(tf.int32, shape = [None], name = \"answer_lengths\")\n",
    "        \n",
    "        self.dropout = tf.placeholder(tf.float32, shape=[], name = \"dropout\")\n",
    "    \n",
    "    def get_feed_dict(self, input_data, dropout_val = 1.0):\n",
    "        \"\"\" Convert input data to list of ids for each sample.\"\"\"\n",
    "        _support_ids = []\n",
    "        _question_ids = []\n",
    "        _distractor1_ids = []\n",
    "        _distractor2_ids = []\n",
    "        _distractor3_ids = []\n",
    "        _answer_ids = []\n",
    "        \n",
    "        for index in range (len(input_data)):\n",
    "            supportTokens = tokenize(train[index]['support'])\n",
    "            questionTokens = tokenize(train[index]['question'])\n",
    "            distractor1Tokens = tokenize(train[index]['distractor1'])\n",
    "            distractor2Tokens = tokenize(train[index]['distractor2'])\n",
    "            distractor3Tokens = tokenize(train[index]['distractor3'])\n",
    "            answerTokens = tokenize(train[index]['correct_answer'])\n",
    "\n",
    "            _support_ids.append([str(self.vocab.get(w, UNK_ID)) for w in supportTokens])\n",
    "            _question_ids.append([str(self.vocab.get(w, UNK_ID)) for w in questionTokens])\n",
    "            _distractor1_ids.append([str(self.vocab.get(w, UNK_ID)) for w in distractor1Tokens])\n",
    "            _distractor2_ids.append([str(self.vocab.get(w, UNK_ID)) for w in distractor2Tokens])\n",
    "            _distractor3_ids.append([str(self.vocab.get(w, UNK_ID)) for w in distractor3Tokens])\n",
    "            _answer_ids.append([str(self.vocab.get(w, UNK_ID)) for w in answerTokens])\n",
    "        \n",
    "        ## Padding\n",
    "        padded_questions, question_lengths = pad_sequences(_question_ids, 0)\n",
    "        padded_support, support_lengths = pad_sequences(_support_ids, 0)\n",
    "        padded_distractor1, distractor1_lengths = pad_sequences(_distractor1_ids, 0)\n",
    "        padded_distractor2, distractor2_lengths = pad_sequences(_distractor2_ids, 0)\n",
    "        padded_distractor3, distractor3_lengths = pad_sequences(_distractor3_ids, 0)\n",
    "        padded_answer, answer_lengths = pad_sequences(_answer_ids, 0)\n",
    "\n",
    "        feed = {\n",
    "            self.question_ids:padded_questions, self.question_lengths:question_lengths,\n",
    "            self.support_ids:padded_support, self.support_lengths:support_lengths,\n",
    "            self.distractor1_ids:padded_distractor1, self.distractor1_lengths:distractor1_lengths,\n",
    "            self.distractor2_ids:padded_distractor1, self.distractor2_lengths:distractor2_lengths,\n",
    "            self.distractor3_ids:padded_distractor1, self.distractor3_lengths:distractor3_lengths,\n",
    "            self.answer_ids:padded_answer, self.answer_lengths:answer_lengths\n",
    "        }\n",
    "        \n",
    "        return feed\n",
    "    \n",
    "    def setup_word_embeddings(self):\n",
    "        '''\n",
    "            Create an embedding matrix (initialised with pretrained glove vectors and updated only if self.config.train_embeddings is true)\n",
    "            lookup into this matrix and apply dropout (which is 1 at test time and self.config.dropout at train time)\n",
    "        '''\n",
    "        with tf.variable_scope(\"vocab_embeddings\"):\n",
    "            question_emb = tf.nn.embedding_lookup(self.embedding, self.question_ids, name = \"question\") # (-1, Q, D)\n",
    "            support_emb = tf.nn.embedding_lookup(self.embedding, self.support_ids, name = \"support\") # (-1, P, D)\n",
    "            distractor1_emb = tf.nn.embedding_lookup(self.embedding, self.distractor1_ids, name = \"distractor1\") # (-1, P, D)\n",
    "            distractor2_emb = tf.nn.embedding_lookup(self.embedding, self.distractor2_ids, name = \"distractor2\")\n",
    "            distractor3_emb = tf.nn.embedding_lookup(self.embedding, self.distractor3_ids, name = \"distractor3\")\n",
    "            answer_emb = tf.nn.embedding_lookup(self.embedding, self.answer_ids, name = \"answer\")\n",
    "            # Apply dropout\n",
    "            self.question = tf.nn.dropout(question_emb, self.dropout)\n",
    "            self.support = tf.nn.dropout(support_emb, self.dropout)\n",
    "            self.distractor1 = tf.nn.dropout(distractor1_emb, self.dropout)\n",
    "            self.distractor2 = tf.nn.dropout(distractor2_emb, self.dropout)\n",
    "            self.distractor3 = tf.nn.dropout(distractor3_emb, self.dropout)\n",
    "            self.answer = tf.nn.dropout(answer_emb, self.dropout)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<tf.Tensor 'question_ids_1:0' shape=(?, ?) dtype=int32>: array([['2', '1554', '3', '18181', '14', '3659', '180', '6', '4789', '3',\n",
      "        '5008', '125', '19', '5795', '5', '19302', '188']], dtype='<U5'), <tf.Tensor 'question_lengths_1:0' shape=(?,) dtype=int32>: array([17]), <tf.Tensor 'support_ids_1:0' shape=(?, ?) dtype=int32>: array([['2', '2274', '254', '6', '3230', '3987', '1', '2681', '118', '2',\n",
      "        '5', '2', '23', '2', '5', '2', '24', '2', '2', '32', '456',\n",
      "        '238', '756', '6', '46', '13', '0', '1741', '3', '3790', '46',\n",
      "        '68', '2430', '2', '2', '17665', '552', '3987', '3', '109',\n",
      "        '33598', '2', '14', '2', '23', '2', '24', '1', '0', '1973',\n",
      "        '473', '719', '3987', '2', '2', '12290', '33', '481', '2054',\n",
      "        '6', '565', '4789', '1', '144', '5795', '1', '19302', '1',\n",
      "        '4178', '5', '2760', '2']], dtype='<U5'), <tf.Tensor 'support_lengths_1:0' shape=(?,) dtype=int32>: array([72]), <tf.Tensor 'distractor1_ids_1:0' shape=(?, ?) dtype=int32>: array([['80954']], dtype='<U5'), <tf.Tensor 'distractor1_lengths_1:0' shape=(?,) dtype=int32>: array([1]), <tf.Tensor 'distractor2_ids_1:0' shape=(?, ?) dtype=int32>: array([['80954']], dtype='<U5'), <tf.Tensor 'distractor2_lengths_1:0' shape=(?,) dtype=int32>: array([1]), <tf.Tensor 'distractor3_ids_1:0' shape=(?, ?) dtype=int32>: array([['80954']], dtype='<U5'), <tf.Tensor 'distractor3_lengths_1:0' shape=(?,) dtype=int32>: array([1]), <tf.Tensor 'answer_ids_1:0' shape=(?, ?) dtype=int32>: array([['312431', '12290']], dtype='<U6'), <tf.Tensor 'answer_lengths_1:0' shape=(?,) dtype=int32>: array([2])}\n"
     ]
    }
   ],
   "source": [
    "feed = pipe.get_feed_dict(train[:1])\n",
    "print(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajnya/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py:135: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if params is None or params in ((), []):\n"
     ]
    }
   ],
   "source": [
    "pipe.setup_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question_embed.append(tf.nn.embedding_lookup(W, np.array(list(vocab_processor.transform(question_ids)))))\n",
    "#     support_embed.append(tf.nn.embedding_lookup(W, np.array(list(vocab_processor.transform(support_ids)))))\n",
    "#     answer_embed = tf.nn.embedding_lookup(W, np.array(list(vocab_processor.transform(answerTokens[index]))))\n",
    "#     distractor1_embed = tf.nn.embedding_lookup(W, np.array(list(vocab_processor.transform(distractor1Tokens[index]))))\n",
    "#     distractor2_embed = tf.nn.embedding_lookup(W, np.array(list(vocab_processor.transform(distractor2Tokens[index]))))\n",
    "#     distractor3_embed = tf.nn.embedding_lookup(W, np.array(list(vocab_processor.transform(distractor3Tokens[index]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mesophiles', 'grow', 'best', 'in', 'moderate', 'temperature', ',', 'typically', 'between', '25°C', 'and', '40°C', '(', '77°F', 'and', '104°F', ')', '.', 'Mesophiles', 'are', 'often', 'found', 'living', 'in', 'or', 'on', 'the', 'bodies', 'of', 'humans', 'or', 'other', 'animals', '.', 'The', 'optimal', 'growth', 'temperature', 'of', 'many', 'pathogenic', 'mesophiles', 'is', '37°C', '(', '98°F', ')', ',', 'the', 'normal', 'human', 'body', 'temperature', '.', 'Mesophilic', 'organisms', 'have', 'important', 'uses', 'in', 'food', 'preparation', ',', 'including', 'cheese', ',', 'yogurt', ',', 'beer', 'and', 'wine', '.']\n"
     ]
    }
   ],
   "source": [
    "some_list = tokenize(train[0]['support'])\n",
    "print(some_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as pp\n",
    "sess = tf.Session()\n",
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
    "                trainable=False, name=\"W\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "embedding_init = W.assign(embedding_placeholder)\n",
    "sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from tensorflow.contrib import learn\n",
    "#init vocab processor\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(len(train))\n",
    "#fit the vocab from glove\n",
    "pretrain = vocab_processor.fit(rev_vocab)\n",
    "#transform inputs\n",
    "for index in range (len(train)):\n",
    "    question_embed = tf.nn.embedding_lookup(W, np.array(list(vocab_processor.transform(questionTokens[index]))))\n",
    "    support_embed = tf.nn.embedding_lookup(W, np.array(list(vocab_processor.transform(supportTokens[index]))))\n",
    "    answer_embed = tf.nn.embedding_lookup(W, np.array(list(vocab_processor.transform(answerTokens[index]))))\n",
    "    distractor1_embed = tf.nn.embedding_lookup(W, np.array(list(vocab_processor.transform(distractor1Tokens[index]))))\n",
    "    distractor2_embed = tf.nn.embedding_lookup(W, np.array(list(vocab_processor.transform(distractor2Tokens[index]))))\n",
    "    distractor3_embed = tf.nn.embedding_lookup(W, np.array(list(vocab_processor.transform(distractor3Tokens[index]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pass tokens (question/answer/support) as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-3ea2b6e15d03>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-3ea2b6e15d03>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    dataset = tf.dataset.\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dataset = tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
