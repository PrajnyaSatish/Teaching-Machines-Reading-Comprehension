{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajnya/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from functools import reduce\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "from pprint import pprint as pp\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    \"\"\"Returns the tokens of a sequece\"\"\"\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'data/glove.6B.50d.txt'\n",
    "# UNK_ID = 2\n",
    "# def loadGloVe(filename):\n",
    "#     vocab_list = []\n",
    "#     embd = []\n",
    "#     file = open(filename,'r')\n",
    "#     for line in file.readlines():\n",
    "#         row = line.strip().split(' ')\n",
    "#         vocab_list.append(row[0])\n",
    "#         embd.append(row[1:])\n",
    "#     print('Loaded GloVe!')\n",
    "#     file.close()\n",
    "#     vocab_size = len(vocab_list)+1\n",
    "    \n",
    "#     vocab = dict((c,i+1) for i, c in enumerate(vocab_list))\n",
    "#     vocab['UNK_ID'] = 2\n",
    "#     return vocab,vocab_list,embd\n",
    "# vocab,vocab_list,embd = loadGloVe(filename)\n",
    "# vocab_size = len(vocab_list)\n",
    "# embedding_dim = len(embd[0])\n",
    "# embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/SciQ dataset/train.json', 'r') as rf:\n",
    "    train = json.load(rf)\n",
    "with open('data/SciQ dataset/test.json', 'r') as rf:\n",
    "    test = json.load(rf)\n",
    "with open('data/SciQ dataset/valid.json', 'r') as rf:\n",
    "    valid = json.load(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = set()\n",
    "for sample in train+test+valid:\n",
    "    s_t = tokenize(sample['support'])\n",
    "    q_t = tokenize(sample['question'])\n",
    "    d1_t = tokenize(sample['distractor1'])\n",
    "    d2_t = tokenize(sample['distractor2'])\n",
    "    d3_t = tokenize(sample['distractor3'])\n",
    "    a_t = tokenize(sample['correct_answer'])\n",
    "    vocab_list |= set(s_t+q_t+d1_t+d2_t+d3_t+a_t)\n",
    "vocab_list=sorted(vocab_list)\n",
    "vocab_size = len(vocab_list)+3\n",
    "vocab = dict((c,i+2) for i,c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(input_sent, vocab, vocab_list):\n",
    "    tokenized = tokenize(input_sent)\n",
    "    vectorized = []\n",
    "    for w in tokenized:\n",
    "        if w in vocab_list:\n",
    "            vectorized.append(vocab[w])\n",
    "        else:\n",
    "            vectorized.append(vocab['UNK_ID'])\n",
    "    return vectorized\n",
    "    \n",
    "\n",
    "def vectorize_input(data, vocab, vocab_size, support_maxlen, query_maxlen):\n",
    "    qs = []\n",
    "    sps = []\n",
    "    ans = []\n",
    "    dis1s = []\n",
    "    dis2s = []\n",
    "    dis3s = []\n",
    "    for _,sample in enumerate(data):\n",
    "        q_vect = get_vectors(sample['question'], vocab, vocab_list)\n",
    "        support_vect = get_vectors(sample['support'], vocab , vocab_list)\n",
    "        ans_tokens = tokenize(sample['correct_answer'])\n",
    "        answer_vect = np.zeros(vocab_size)\n",
    "        for w in ans_tokens:\n",
    "            if w in vocab_list:\n",
    "                answer_vect[vocab[w]]=1\n",
    "            else:\n",
    "                answer_vect[vocab['UNK_ID']]=1\n",
    "                \n",
    "        answer_vect = np.transpose(answer_vect)\n",
    "                    \n",
    "                                \n",
    "        distractor1_vect = get_vectors(sample['distractor1'], vocab , vocab_list)\n",
    "        distractor2_vect = get_vectors(sample['distractor2'], vocab , vocab_list)\n",
    "        distractor3_vect = get_vectors(sample['distractor3'], vocab , vocab_list)\n",
    "        qs.append(q_vect)\n",
    "        sps.append(support_vect)\n",
    "        ans.append(answer_vect)\n",
    "        dis1s.append(distractor1_vect)\n",
    "        dis2s.append(distractor2_vect)\n",
    "        dis3s.append(distractor3_vect)\n",
    "    return(pad_sequences(qs, maxlen=query_maxlen),\\\n",
    "           pad_sequences(sps, maxlen=support_maxlen),\\\n",
    "           np.array(ans),\\\n",
    "           pad_sequences(dis1s, maxlen=query_maxlen),\\\n",
    "           pad_sequences(dis2s, maxlen=query_maxlen),\\\n",
    "           pad_sequences(dis3s, maxlen=query_maxlen)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#vectorize_input(train[:5], vocab, vocab_list, 500, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM/EMBED/SUPPORT/QUERY=<class 'keras.layers.recurrent.LSTM'>,300,300,100\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 300\n",
    "Q_HIDDEN_SIZE = 100\n",
    "S_HIDDEN_SIZE = 300\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "print('LSTM/EMBED/SUPPORT/QUERY={0},{1},{2},{3}'.format(LSTM,\n",
    "                                                    EMBED_SIZE,\n",
    "                                                    S_HIDDEN_SIZE,\n",
    "                                                    Q_HIDDEN_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q,s,a,d1,d2,d3 = vectorize_input(train, vocab, vocab_size, S_HIDDEN_SIZE, Q_HIDDEN_SIZE)\n",
    "support = layers.Input(shape=(S_HIDDEN_SIZE,), dtype='int32')\n",
    "encoded_support = layers.Embedding(vocab_size, EMBED_SIZE)(support)\n",
    "encoded_support = layers.Dropout(0.3)(encoded_support)\n",
    "support_LSTM = LSTM(EMBED_SIZE)(encoded_support)\n",
    "support_LSTM = layers.RepeatVector(S_HIDDEN_SIZE)(support_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = layers.Input(shape=(Q_HIDDEN_SIZE,), dtype='int32')\n",
    "encoded_question = layers.Embedding(vocab_size, EMBED_SIZE)(question)\n",
    "encoded_question = layers.Dropout(0.3)(encoded_question)\n",
    "question_LSTM = LSTM(EMBED_SIZE)(encoded_question)\n",
    "question_LSTM = layers.RepeatVector(S_HIDDEN_SIZE)(question_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = layers.add([support_LSTM, question_LSTM])\n",
    "match_LSTM = LSTM(EMBED_SIZE)(merged)\n",
    "match_LSTM = layers.Dropout(0.3)(match_LSTM)\n",
    "predictions = layers.Dense(vocab_size, activation='softmax')(match_LSTM)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([support, question], predictions)\n",
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajnya/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 120000000 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    }
   ],
   "source": [
    "print('Training')\n",
    "model.fit([s,q], a,\n",
    "         batch_size=BATCH_SIZE,\n",
    "         epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
